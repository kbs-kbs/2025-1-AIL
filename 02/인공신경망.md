# 사인 함수 예측하기
## 파이토치 학습 과정
1. 모델 정의
2. 데이터 불러오기
3. 손실 계산
4. 오차 역전파 및 최적화
5. 3~4 반복
6. 학습 종료

## 모델 아키텍처 정의와 초기화
```python
a = torch.randn()
b = torch.randn()
c = torch.randn()
d = torch.randn()
y_random = a*x**3 + b*x**2 + c*x + d
```

> [!note]
> 파이썬의 일반 랜덤 함수로 파라미터를 만들면, 딥러닝 학습이 불가능합니다.   
> PyTorch 텐서로 파라미터를 만들어야 학습(역전파, 파라미터 업데이트)이 가능합니다.   
> 왜냐하면 `y_pred = a * x**3 + b * x**2 + c * x + d` 계산 시 결과가 상수가 되기 때문   
> 오차 역전파 시 예측값은 식(계산 그래프의 노드) 형태를 유지해야 하는데 이걸 파이토치가 해줌

## 모델 학습
```python
learning_rate = 1e-6 # 학습률 정의

for epoch in range(2000): # 학습 2000번 진행
  y_pred = a*x**3 + b*x**2 + c*x + d

  # SSE 손실 함수 정의, MSE를 사용하려면 sum()을 mean()으로 바꾸기
  loss = (y_pred - y).pow(2).sum()

  # 합성 함수인 손실 함수의 편미분:
  # Σ(∂((y_pred - y)**2)/∂(y_pred - y) * ∂(y_pred - y)/∂(a|b|c|d))
  grad_y_pred = 2.0 * (y_pred - y) # 기울기의 미분값
  grad_a = (grad_y_pred * x ** 3).sum()
  grad_b = (grad_y_pred * x ** 2).sum()
  grad_c = (grad_y_pred * x).sum()
  grad_d = grad_y_pred.sum()

  # 가중치 업데이트 (기울기의 반대방향으로 이동 = 손실이 작아지는 방향으로 이동)
  a -= learning_rate * grad_a 
  b -= learning_rate * grad_b
  c -= learning_rate * grad_c
  d -= learning_rate * grad_d
```

> [!note]
> 선형 회귀 = (활성화 함수 없는) 단층 신경망   
> 즉, 선형 회귀 모델은 은닉층이 없는 신경망과 수학적으로 동일합니다     
> 활성화 함수는 오차 계산 전에 실행됩니다.      
> 아달라인은 선형 회귀에 양자화(임계값 적용) 단계가 추가된 것으로 설명할 수 있습니다.   
> (단, 이 양자화는 학습이 아니라 예측/분류 결과를 해석할 때만 적용됩니다.)    
> 퍼셉트론의 경우, 활성화 함수가 곧 양자화 함수(계단 함수) 그 자체라고 볼 수 있습니다    
> 이 코드는 입력값 x를 여러 다항 특성(x³, x², x, 1)으로 확장해서   
> 여러 특성을 사용하는 것과 같은 효과를 냅니다.    
> 즉, 입력값이 여러 개인 모델(x₁, x₂, x₃, ...)과 수식적으로 동일하게 동작합니다    

# 집값 예측하기
```python
# 모델 정의
model = nn.Sequential(
  nn.Linear(13, 100), # 입력 노드 13개 은닉 노드 100개
  nn.ReLU(), # 0 이상 값만 유지
  nn.Linear(100, 1) # 출력 노드 1개
)
```

## 최적화 함수 정의
```
# 경사하강법에서 발전된 Adam 사용
# 파라미터마다 가중치를 조정해 주어야 하기 때문에 파라미터 넘김
optim = Adam(model.parameters(), lr=learning_rate)
```

## 배치 사용하여 학습
```
# 에포크 반복
for epoch in range(200):
  # 배치 반복(마지막 batch까지 모두 학습하도록 코드를 수정함)
  for i in range(math.ceil(len(X)/batch_size)):
    start = i*batch_size      # 배치 크기에 맞게 인덱스를 지정
    end = start + batch_size

    # 파이토치 실수형 텐서로 변환
    x = torch.FloatTensor(X[start:end])
    y = torch.FloatTensor(Y[start:end])

    optim.zero_grad() # 가중치의 기울기를 0으로 초기화
    preds = model(x)  # 모델의 예측값 계산
    loss = nn.MSELoss()(preds, y) # MSE 손실 계산 객체 생성과 동시에 사용 (`__call__()`의 인자)
    loss.backward() # 오차 역전파
    optim.step() # 가중치 조정
```

# 손글씨 분류하기
## 데이터 분리
```python
training_data = MNIST(root="./", train=True, download=True, transform=ToTensor())
test_data = MNIST(root="./", train=False, download=True, transform=ToTensor())
```

## 데이터 로더 (배치 정의)
```python
#MNIST 데이터셋은 train/test 데이터가 나뉘어서 제공됨
train_loader = DataLoader(training_data, batch_size=32, shuffle=True)
#평가용은 데이터를 섞을 필요가 없음
test_loader = DataLoader(test_data, batch_size=32, shuffle=False)
```

## 모델 아키텍처 정의
```python
model = nn.Sequential(
  nn.Linear(784, 64), #fully connected layer
  nn.ReLU(),
  nn.Linear(64, 64), #fully connected layer
  nn.ReLU(),
  nn.Linear(64, 10) #fully connected layer
)
model.to(device) # 모델의 파라미터를 GPU(or CPU)로 보냄
```

## 학습
```python
lr = 1e-3
optim = Adam(model.parameters(), lr=lr)

for epoch in range(20):
  for data, label in train_loader:
    optim.zero_grad()
    # 입력 데이터를 모델의 입력에 맞게 모양을 변환 (평탄화)
    data = torch.reshape(data, (-1, 784)).to(device)
    preds = model(data)

    loss = nn.CrossEntropyLoss()(preds, label.to(device)) # 손실 계산
    loss.backward()
    optim.step()
```

> [!note]
> nn.CrossEntropyLoss()는 **입력값이 softmax를 거치지 않은 "로짓(logit, 점수)"**이기를 기대합니다.
> 내부적으로 softmax 연산과 negative log likelihood 연산을 모두 포함하고 있습니다.
> 따라서, 마지막 레이어의 출력이 원-핫 벡터가 아니고, 활성화 함수가 없어도 됩니다.
> 모델의 마지막 레이어 출력은 nn.Linear(64, 10)(로짓)로 하면 됨
