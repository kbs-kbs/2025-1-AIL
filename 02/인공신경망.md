# 사인 함수 예측하기
## 파이토치 학습 과정
1. 모델 정의
2. 모델 순전파(결과는 계산 그래프인 y_pred)
3. 오차 계산
4. 오차 역전파
5. 2~4 반복
6. 학습 종료

## 모델 아키텍처 정의와 초기화
```python
a = torch.randn()
b = torch.randn()
c = torch.randn()
d = torch.randn()
y_random = a*x**3 + b*x**2 + c*x + d
```

> [!note]
> 파이썬의 일반 랜덤 함수로 파라미터를 만들면, 딥러닝 학습이 불가능합니다.   
> PyTorch 텐서로 파라미터를 만들어야 학습(역전파, 파라미터 업데이트)이 가능합니다.   
> 왜냐하면 `y_pred = a * x**3 + b * x**2 + c * x + d` 계산 시 결과가 상수가 되기 때문   
> 오차 역전파 시 예측값은 식(계산 그래프의 노드) 형태를 유지해야 하는데 이걸 파이토치가 해줌

## 모델 학습
```python
learning_rate = 1e-6 # 학습률 정의

for epoch in range(2000): # 학습 2000번 진행
  y_pred = a*x**3 + b*x**2 + c*x + d

  # SSE 손실 함수 정의, MSE를 사용하려면 sum()을 mean()으로 바꾸기
  loss = (y_pred - y).pow(2).sum()

  # 합성 함수인 손실 함수의 편미분:
  # Σ(∂((y_pred - y)**2)/∂(y_pred - y) * ∂(y_pred - y)/∂(a|b|c|d))
  grad_y_pred = 2.0 * (y_pred - y) # 기울기의 미분값
  grad_a = (grad_y_pred * x ** 3).sum()
  grad_b = (grad_y_pred * x ** 2).sum()
  grad_c = (grad_y_pred * x).sum()
  grad_d = grad_y_pred.sum()

  # 가중치 업데이트 (기울기의 반대방향으로 이동 = 손실이 작아지는 방향으로 이동)
  a -= learning_rate * grad_a 
  b -= learning_rate * grad_b
  c -= learning_rate * grad_c
  d -= learning_rate * grad_d
```

> [!note]
> 선형 회귀 = (활성화 함수 없는) 단층 신경망   
> 즉, 선형 회귀 모델은 은닉층이 없는 신경망과 수학적으로 동일합니다     
> 활성화 함수는 오차 계산 전에 실행됩니다.      
> 아달라인은 선형 회귀에 양자화(임계값 적용) 단계가 추가된 것으로 설명할 수 있습니다.   
> (단, 이 양자화는 학습이 아니라 예측/분류 결과를 해석할 때만 적용됩니다.)    
> 퍼셉트론의 경우, 활성화 함수가 곧 양자화 함수(계단 함수) 그 자체라고 볼 수 있습니다    
> 이 코드는 입력값 x를 여러 다항 특성(x³, x², x, 1)으로 확장해서   
> 여러 특성을 사용하는 것과 같은 효과를 냅니다.    
> 즉, 입력값이 여러 개인 모델(x₁, x₂, x₃, ...)과 수식적으로 동일하게 동작합니다    

# 집값 예측하기
# 손글씨 분류하기
